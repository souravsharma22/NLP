{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus= '''My name is optimus Prime.\n",
    "I am the leader of autobots!\n",
    "MY name is Clark kent and i am superman.\n",
    "I am BATMAN it is batman's world.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is optimus Prime.\n",
      "I am the leader of autobots!\n",
      "MY name is Clark kent and i am superman.\n",
      "I am BATMAN it is batman's world.\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "document = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is optimus Prime.\n",
      "I am the leader of autobots!\n",
      "MY name is Clark kent and i am superman.\n",
      "I am BATMAN it is batman's world.\n"
     ]
    }
   ],
   "source": [
    "for token in document:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'optimus',\n",
       " 'Prime',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'the',\n",
       " 'leader',\n",
       " 'of',\n",
       " 'autobots',\n",
       " '!',\n",
       " 'MY',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Clark',\n",
       " 'kent',\n",
       " 'and',\n",
       " 'i',\n",
       " 'am',\n",
       " 'superman',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'BATMAN',\n",
       " 'it',\n",
       " 'is',\n",
       " 'batman',\n",
       " \"'s\",\n",
       " 'world',\n",
       " '.']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'optimus', 'Prime', '.']\n",
      "['I', 'am', 'the', 'leader', 'of', 'autobots', '!']\n",
      "['MY', 'name', 'is', 'Clark', 'kent', 'and', 'i', 'am', 'superman', '.']\n",
      "['I', 'am', 'BATMAN', 'it', 'is', 'batman', \"'s\", 'world', '.']\n"
     ]
    }
   ],
   "source": [
    "for token in document:\n",
    "    print(word_tokenize(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize, TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'optimus', 'Prime', '.']\n",
      "['I', 'am', 'the', 'leader', 'of', 'autobots', '!']\n",
      "['MY', 'name', 'is', 'Clark', 'kent', 'and', 'i', 'am', 'superman', '.']\n",
      "['I', 'am', 'BATMAN', 'it', 'is', 'batman', \"'\", 's', 'world', '.']\n"
     ]
    }
   ],
   "source": [
    "for token in document:\n",
    "    print(wordpunct_tokenize(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M y   n a m e   i s   o p t i m u s   P r i m e.\n",
      "I   a m   t h e   l e a d e r   o f   a u t o b o t s!\n",
      "M Y   n a m e   i s   C l a r k   k e n t   a n d   i   a m   s u p e r m a n.\n",
      "I   a m   B A T M A N   i t   i s   b a t m a n' s   w o r l d.\n"
     ]
    }
   ],
   "source": [
    "tokeniser = TreebankWordDetokenizer()\n",
    "for token in document:\n",
    "    print(tokeniser.tokenize(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'optimus', 'Prime', '.']\n",
      "['I', 'am', 'the', 'leader', 'of', 'autobots', '!']\n",
      "['MY', 'name', 'is', 'Clark', 'kent', 'and', 'i', 'am', 'superman', '.']\n",
      "['I', 'am', 'BATMAN', 'it', 'is', 'batman', \"'s\", 'world', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokener1 = TreebankWordTokenizer()\n",
    "for token in document:\n",
    "    print(tokener1.tokenize(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['eating','eaten','eat', 'writing','writes','programming','programs','going','goes','go','finally','finalized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating-------->eat\n",
      "eaten-------->eaten\n",
      "eat-------->eat\n",
      "writing-------->write\n",
      "writes-------->write\n",
      "programming-------->program\n",
      "programs-------->program\n",
      "going-------->go\n",
      "goes-------->goe\n",
      "go-------->go\n",
      "finally-------->final\n",
      "finalized-------->final\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word +\"-------->\"+ stemer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratul'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemer.stem('Congratulations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "stemer_1 = RegexpStemmer('ing$|es$|s$|e$|able$',min=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemer_1.stem(\"eating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating-------> eat\n",
      "eaten-------> eaten\n",
      "eat-------> eat\n",
      "writing-------> writ\n",
      "writes-------> writ\n",
      "programming-------> programm\n",
      "programs-------> program\n",
      "going-------> go\n",
      "goes-------> go\n",
      "go-------> go\n",
      "finally-------> finally\n",
      "finalized-------> finalized\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word +\"-------> \"+stemer_1.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SnowBall Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer2 = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating-------> eat\n",
      "eaten-------> eaten\n",
      "eat-------> eat\n",
      "writing-------> write\n",
      "writes-------> write\n",
      "programming-------> program\n",
      "programs-------> program\n",
      "going-------> go\n",
      "goes-------> goe\n",
      "go-------> go\n",
      "finally-------> final\n",
      "finalized-------> final\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word +\"-------> \"+stemmer2.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sportingly'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemer_1.stem('Sportingly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sport'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer2.stem('Sportingly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stem'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer2.stem(\"Stemming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stemm'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemer_1.stem('Stemming')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
